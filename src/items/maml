#date 2019-02-26T12:12:27
#title Model Agnostic Meta Learning (MAML)

Model-Agnostic Meta-Learning (MAML, [Finn et al. (2017)](?ii=finn2017model)) is an algorithm for multi-task settings which conditions a set of parameters to be quickly adaptable to a new task via gradient descent.

Formally, MAML optimizes $\theta$ such that

$$\mathcal{L}_{\mathcal{T}\sim p(\mathcal{T})}(\theta - U(\theta, k))$$

is minimized, where typically $U$ is $k$ steps of [gradient descent](?ii=sgd):
$$U(\theta, k) = \alpha \sum_{i=1}^k \nabla_{\theta_{i-1}} \mathcal{L}_{\mathcal{T}\sim p(\mathcal{T})}(\theta_{i-1})$$
$$\theta_0 = \theta;\; \theta_i = \theta - U(\theta, i)$$
and $p(\mathcal{T})$ is a distribution of tasks.


Intuitively, because MAML effectively optimizes for $\theta$ instead of the inner $\theta_i$, it conditions the parameters of the model to have a low loss when updated, but also to then be able to be quickly updated ($k$ is typically low, e.g. 1 to 5 steps).