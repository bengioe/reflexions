#date 2019-02-28T12:04:48
#title Error-based latent volumes

Neil has this idea which is, roughly, to squeeze reprentations so that things that have a "large error" take as much "space" than they have error. Neil's view is that it eliminates outliers by effectively "compressing" normal stuff in a smaller representational volume.

Let's try to tackle this a bit more formally.

We encode $X \sim \mathcal{X}$ into a latent space $\mathcal{H}$ imperfectly. This could be measured through reconstruction error, $H = f(X)$, $r = \|X - f^{-1}(f(X))\|$.

If we naively minimize $r$, and there are a lot ($N_0$) of "easy" examples for which $r = \epsilon$ but very few $n_0$ hard examples for which $r = \alpha >> 0$ then the optimal function approximator may choose the parameters for which $\epsilon N_0 + \alpha n_0$ is minimal.

We (I) don't know how neural networks "attribute capacity" to their inputs, or if this analogy is even useful, but let's suppose it is.

What would happen if instead of minimizing error we minimize the maximal error? Or the soft[arg]max?

Let $\tau$ be some temperature. What happens if we write the empirical loss as the maximization of the entropy of the softmax of sublosses? Or the minimization of the convex combination of the loss by its categorical distribution? (i.e. softmax)