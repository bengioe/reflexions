{"asides": [], "title": "Model Agnostic Meta Learning (MAML)", "name": "maml", "date": "2019-02-26T12:12:27", "body": "<p>Model-Agnostic Meta-Learning (MAML, <a href=\"?ii=finn2017model\">Finn et al. (2017)</a>) is an algorithm for multi-task settings which conditions a set of parameters to be quickly adaptable to a new task via gradient descent.</p>\n\n<p>Formally, MAML optimizes $\\theta$ such that</p>\n\n<p>$$\\mathcal{L}_{\\mathcal{T}\\sim p(\\mathcal{T})}(\\theta - U(\\theta, k))$$</p>\n\n<p>is minimized, where typically $U$ is $k$ steps of <a href=\"?ii=sgd\">gradient descent</a>:\n$$U(\\theta, k) = \\alpha \\sum_{i=1}^k \\nabla_{\\theta_{i-1}} \\mathcal{L}_{\\mathcal{T}\\sim p(\\mathcal{T})}(\\theta_{i-1})$$\n$$\\theta_0 = \\theta;\\; \\theta_i = \\theta - U(\\theta, i)$$\nand $p(\\mathcal{T})$ is a distribution of tasks.</p>\n\n<p>Intuitively, because MAML effectively optimizes for $\\theta$ instead of the inner $\\theta_i$, it conditions the parameters of the model to have a low loss when updated, but also to then be able to be quickly updated ($k$ is typically low, e.g. 1 to 5 steps).</p>\n"}